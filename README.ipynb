{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "논문 제목, 프로젝트 간략설명\n",
    "\n",
    "시스템 환경(파이썬 버전 등등)\n",
    "\n",
    "data 설명(view 하는것까지)\n",
    "\n",
    "Main code 실행법\n",
    "\n",
    "중간 결과\n",
    "\n",
    "구현해야될것들 등등 부가설명\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Estimate 3D Hand Pose from Single RGB Images\n",
    "\n",
    "Clova AI Intern Interview Assignment\n",
    "\n",
    "Implementation of [Learning to Estimate 3D Hand Pose from Single RGB Images][paper1] by tensorflow.\n",
    "\n",
    "## Recommended Python Packages Version\n",
    "\n",
    "* python : 3.5.6\n",
    "* tensorflow : 1.10.0\n",
    "* numpy : 1.18.5\n",
    "* matplotlib : 3.0.0\n",
    "* opencv : 3.4.1\n",
    "\n",
    "\n",
    "## Data Set\n",
    "\n",
    "![datasample](./data/data_sample.PNG)\n",
    "### [Rendered Handpose Dataset](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html)\n",
    "\n",
    "This dataset provides 41258 training and 2728 testing samples. Each sample provides:\n",
    "- RGB image (320x320 pixels)\n",
    "- Depth map (320x320 pixels); See examples for how to convert the provided RGB\n",
    "- Segmentation masks (320x320 pixels) for the following classes: background, person, three classes for each finger and one for each palm\n",
    "- 21 Keypoints for each hand with their uv coordinates in the image frame, xyz coordinates in the world frame and an indicator if they are visible\n",
    "- Intrinsic Camera Matrix K\n",
    "It was created with freely available character from www.mixamo.com and rendered with www.blender.org\n",
    "For more details see the aforementioned paper.\n",
    "\n",
    "### Keypoints available:\n",
    "* 0: left wrist\n",
    "* 1-4: left thumb(tip to palm)\n",
    "* 5-8: left index, ...17-20: left pinky\n",
    "* 21: right wrist\n",
    "* 22-25: right thumb, ..., 38-41: right pinky\n",
    "\n",
    "### Segmentation masks available:\n",
    "* 0: background\n",
    "* 1: person, \n",
    "* 2-4: left thumb(tip to palm)\n",
    "* 5-7: left index, ..., 14-16: left pinky, 17: palm\n",
    "* 18-20: right thumb, ..., 33: right palm\n",
    "\n",
    "### Load data\n",
    "\n",
    "* Download [data](https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html) and set 'path_to_db' at loader.py to where the dataset is located on your machine\n",
    "\n",
    "* Optionally modify data_set variable to training or evaluation\n",
    "``` python\n",
    "# loader.py\n",
    "path_to_db = './RHD_published_v2/'\n",
    "tloader = loader.Data('training')\n",
    "eloader = loader.Data('evaluation')\n",
    "```\n",
    "* You can open data sample by sample_id\n",
    "``` python\n",
    "# loader.py\n",
    "sample_id = 3\n",
    "image, mask, depth = tloader.load_id(sample_id)\n",
    "anno = tloader.anno_all[sample_id]\n",
    "``` \n",
    "\n",
    "## Training Network\n",
    "The model saved automatically when it is initialized. If there is a same model ID(i.e. name) on it's directory, the remain one can be removed. if you want to load model, you need to use diffrent name for model\n",
    "\n",
    "The model need configuration infro to initialize.\n",
    "\n",
    "    <Configuration info>\n",
    "    ID : Model ID\n",
    "    n_iter : Total number of iterations\n",
    "    n_prt : Loss print cycle\n",
    "    input_h : Image height\n",
    "    input_w : Image width\n",
    "    input_ch : Image channel (e.g. RGB)\n",
    "    n_output : Dimension of output\n",
    "    n_batch : Size of batch\n",
    "    n_save : Model save cycle\n",
    "    n_history : Train/Test loss save cycle\n",
    "    LR : Learning rate\n",
    "    random_crop : Random crop by 256x256 when training HandSegNet\n",
    "    training : True or False, it will determine dropout condition\n",
    "    \n",
    "    <Configuration example>\n",
    "    config = {\n",
    "        'ID' : 'test_handseg',\n",
    "        'n_iter' : 20000,\n",
    "        'n_prt' : 100,\n",
    "        'input_h' : 320,\n",
    "        'input_w' : 320,\n",
    "        'input_ch' : 3,\n",
    "        'n_output' : 10,\n",
    "        'n_batch' : 8,\n",
    "        'n_save' : 1000,\n",
    "        'n_history' : 50,\n",
    "        'LR' : 1e-5,\n",
    "        'random_crop' : True,\n",
    "        'training' : True,\n",
    "    }\n",
    "    \n",
    "### Example of training network\n",
    "``` python\n",
    "import loader\n",
    "import model\n",
    "\n",
    "training_loader = loader.Data('training')\n",
    "\n",
    "config = {\n",
    "    'ID' : 'handseg_01',\n",
    "    'n_iter' : 20000,\n",
    "    'n_prt' : 100,\n",
    "    'input_h' : 320,\n",
    "    'input_w' : 320,\n",
    "    'input_ch' : 3,\n",
    "    'n_output' : 10,\n",
    "    'n_batch' : 8,\n",
    "    'n_save' : 1000,\n",
    "    'n_history' : 50,\n",
    "    'LR' : 1e-5,\n",
    "    'random_crop' : True,\n",
    "    'training' : True,\n",
    "}\n",
    "\n",
    "SegNet = model.Hand3DPoseNet(config=config)\n",
    "SegNet.train_HadSegNet(training_loader)\n",
    "```\n",
    "\n",
    "## Loading Network\n",
    "\n",
    "You can simply load the model by checkpoint\n",
    "\n",
    "### Example of loading network\n",
    "\n",
    "``` python\n",
    "config_test = {\n",
    "    'ID' : 'test_model',\n",
    "    'n_iter' : 20000,\n",
    "    'n_prt' : 100,\n",
    "    'input_h' : 320,\n",
    "    'input_w' : 320,\n",
    "    'input_ch' : 3,\n",
    "    'n_output' : 10,\n",
    "    'n_batch' : 8,\n",
    "    'n_save' : 1000,\n",
    "    'n_history' : 50,\n",
    "    'LR' : 1e-5,\n",
    "    'random_crop' : False,\n",
    "    'training' : False,\n",
    "}\n",
    "SegNet_test = model.Hand3DPoseNet(config=config_test)\n",
    "SegNet_test.load(checkpoint) # (e.g) checkpoint = './handseg_01/checkpoint/handseg_01_20000'\n",
    "```\n",
    "\n",
    "[paper1]: https://arxiv.org/pdf/1705.01389.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
